{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 7460,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013404825737265416,
      "grad_norm": 89.71945190429688,
      "learning_rate": 0.009865951742627346,
      "loss": 1.2499,
      "step": 100
    },
    {
      "epoch": 0.02680965147453083,
      "grad_norm": 374.4515380859375,
      "learning_rate": 0.009731903485254692,
      "loss": 1.4074,
      "step": 200
    },
    {
      "epoch": 0.040214477211796246,
      "grad_norm": 256.2913513183594,
      "learning_rate": 0.009597855227882037,
      "loss": 1.5086,
      "step": 300
    },
    {
      "epoch": 0.05361930294906166,
      "grad_norm": 206.9220733642578,
      "learning_rate": 0.009463806970509383,
      "loss": 1.4936,
      "step": 400
    },
    {
      "epoch": 0.06702412868632708,
      "grad_norm": 330.2623596191406,
      "learning_rate": 0.009329758713136729,
      "loss": 1.5586,
      "step": 500
    },
    {
      "epoch": 0.08042895442359249,
      "grad_norm": 89.11042022705078,
      "learning_rate": 0.009195710455764075,
      "loss": 1.4711,
      "step": 600
    },
    {
      "epoch": 0.0938337801608579,
      "grad_norm": 179.57424926757812,
      "learning_rate": 0.009061662198391421,
      "loss": 1.4475,
      "step": 700
    },
    {
      "epoch": 0.10723860589812333,
      "grad_norm": 1069.54638671875,
      "learning_rate": 0.008927613941018767,
      "loss": 1.48,
      "step": 800
    },
    {
      "epoch": 0.12064343163538874,
      "grad_norm": 488.97735595703125,
      "learning_rate": 0.008793565683646113,
      "loss": 1.4329,
      "step": 900
    },
    {
      "epoch": 0.13404825737265416,
      "grad_norm": 270.2605895996094,
      "learning_rate": 0.00865951742627346,
      "loss": 1.42,
      "step": 1000
    },
    {
      "epoch": 0.14745308310991956,
      "grad_norm": 216.97140502929688,
      "learning_rate": 0.008525469168900804,
      "loss": 1.4465,
      "step": 1100
    },
    {
      "epoch": 0.16085790884718498,
      "grad_norm": 909.9922485351562,
      "learning_rate": 0.00839142091152815,
      "loss": 1.4331,
      "step": 1200
    },
    {
      "epoch": 0.1742627345844504,
      "grad_norm": 222.47483825683594,
      "learning_rate": 0.008257372654155496,
      "loss": 1.4326,
      "step": 1300
    },
    {
      "epoch": 0.1876675603217158,
      "grad_norm": 590.811279296875,
      "learning_rate": 0.008123324396782842,
      "loss": 1.4291,
      "step": 1400
    },
    {
      "epoch": 0.20107238605898123,
      "grad_norm": 1239.5008544921875,
      "learning_rate": 0.007989276139410188,
      "loss": 1.4227,
      "step": 1500
    },
    {
      "epoch": 0.21447721179624665,
      "grad_norm": 251.3395233154297,
      "learning_rate": 0.007855227882037533,
      "loss": 1.4286,
      "step": 1600
    },
    {
      "epoch": 0.22788203753351208,
      "grad_norm": 471.6537170410156,
      "learning_rate": 0.007721179624664879,
      "loss": 1.4235,
      "step": 1700
    },
    {
      "epoch": 0.24128686327077747,
      "grad_norm": 882.3735961914062,
      "learning_rate": 0.007587131367292225,
      "loss": 1.4339,
      "step": 1800
    },
    {
      "epoch": 0.2546916890080429,
      "grad_norm": 634.0413818359375,
      "learning_rate": 0.007453083109919572,
      "loss": 1.4189,
      "step": 1900
    },
    {
      "epoch": 0.2680965147453083,
      "grad_norm": 535.3726806640625,
      "learning_rate": 0.007319034852546917,
      "loss": 1.4152,
      "step": 2000
    },
    {
      "epoch": 0.28150134048257375,
      "grad_norm": 4085.99365234375,
      "learning_rate": 0.007184986595174263,
      "loss": 1.4143,
      "step": 2100
    },
    {
      "epoch": 0.2949061662198391,
      "grad_norm": 2413.68115234375,
      "learning_rate": 0.007050938337801609,
      "loss": 1.4185,
      "step": 2200
    },
    {
      "epoch": 0.30831099195710454,
      "grad_norm": 2575.715576171875,
      "learning_rate": 0.0069168900804289545,
      "loss": 1.404,
      "step": 2300
    },
    {
      "epoch": 0.32171581769436997,
      "grad_norm": 1607.196044921875,
      "learning_rate": 0.006782841823056301,
      "loss": 1.4069,
      "step": 2400
    },
    {
      "epoch": 0.3351206434316354,
      "grad_norm": 540.9946899414062,
      "learning_rate": 0.006648793565683646,
      "loss": 1.4136,
      "step": 2500
    },
    {
      "epoch": 0.3485254691689008,
      "grad_norm": 1924.627685546875,
      "learning_rate": 0.006514745308310992,
      "loss": 1.4153,
      "step": 2600
    },
    {
      "epoch": 0.36193029490616624,
      "grad_norm": 835.597900390625,
      "learning_rate": 0.006380697050938338,
      "loss": 1.4056,
      "step": 2700
    },
    {
      "epoch": 0.3753351206434316,
      "grad_norm": 8933.359375,
      "learning_rate": 0.006246648793565683,
      "loss": 1.4033,
      "step": 2800
    },
    {
      "epoch": 0.38873994638069703,
      "grad_norm": 1230.8250732421875,
      "learning_rate": 0.006112600536193029,
      "loss": 1.4163,
      "step": 2900
    },
    {
      "epoch": 0.40214477211796246,
      "grad_norm": 3011.19091796875,
      "learning_rate": 0.005978552278820375,
      "loss": 1.3965,
      "step": 3000
    },
    {
      "epoch": 0.4155495978552279,
      "grad_norm": 29272.232421875,
      "learning_rate": 0.005844504021447721,
      "loss": 1.411,
      "step": 3100
    },
    {
      "epoch": 0.4289544235924933,
      "grad_norm": 2284.903564453125,
      "learning_rate": 0.005710455764075068,
      "loss": 1.4059,
      "step": 3200
    },
    {
      "epoch": 0.44235924932975873,
      "grad_norm": 10112.80859375,
      "learning_rate": 0.005576407506702414,
      "loss": 1.4064,
      "step": 3300
    },
    {
      "epoch": 0.45576407506702415,
      "grad_norm": 2697.490234375,
      "learning_rate": 0.005442359249329759,
      "loss": 1.4043,
      "step": 3400
    },
    {
      "epoch": 0.4691689008042895,
      "grad_norm": 1664.8905029296875,
      "learning_rate": 0.005308310991957105,
      "loss": 1.4073,
      "step": 3500
    },
    {
      "epoch": 0.48257372654155495,
      "grad_norm": 1955.5650634765625,
      "learning_rate": 0.00517426273458445,
      "loss": 1.4022,
      "step": 3600
    },
    {
      "epoch": 0.4959785522788204,
      "grad_norm": 3453.9599609375,
      "learning_rate": 0.0050402144772117964,
      "loss": 1.4043,
      "step": 3700
    },
    {
      "epoch": 0.5093833780160858,
      "grad_norm": 6266.63623046875,
      "learning_rate": 0.0049061662198391425,
      "loss": 1.4108,
      "step": 3800
    },
    {
      "epoch": 0.5227882037533512,
      "grad_norm": 5057.60791015625,
      "learning_rate": 0.004772117962466488,
      "loss": 1.4074,
      "step": 3900
    },
    {
      "epoch": 0.5361930294906166,
      "grad_norm": 2185.850830078125,
      "learning_rate": 0.004638069705093834,
      "loss": 1.4144,
      "step": 4000
    },
    {
      "epoch": 0.5495978552278821,
      "grad_norm": 1543.306640625,
      "learning_rate": 0.00450402144772118,
      "loss": 1.404,
      "step": 4100
    },
    {
      "epoch": 0.5630026809651475,
      "grad_norm": 9019.818359375,
      "learning_rate": 0.004369973190348526,
      "loss": 1.399,
      "step": 4200
    },
    {
      "epoch": 0.5764075067024129,
      "grad_norm": 2812.687744140625,
      "learning_rate": 0.004235924932975871,
      "loss": 1.4037,
      "step": 4300
    },
    {
      "epoch": 0.5898123324396782,
      "grad_norm": 4523.9931640625,
      "learning_rate": 0.004101876675603217,
      "loss": 1.4093,
      "step": 4400
    },
    {
      "epoch": 0.6032171581769437,
      "grad_norm": 9398.177734375,
      "learning_rate": 0.003967828418230563,
      "loss": 1.4017,
      "step": 4500
    },
    {
      "epoch": 0.6166219839142091,
      "grad_norm": 4258.89794921875,
      "learning_rate": 0.0038337801608579088,
      "loss": 1.4111,
      "step": 4600
    },
    {
      "epoch": 0.6300268096514745,
      "grad_norm": 1134.4217529296875,
      "learning_rate": 0.003699731903485255,
      "loss": 1.4121,
      "step": 4700
    },
    {
      "epoch": 0.6434316353887399,
      "grad_norm": 16432.255859375,
      "learning_rate": 0.003565683646112601,
      "loss": 1.4104,
      "step": 4800
    },
    {
      "epoch": 0.6568364611260054,
      "grad_norm": 2093.247314453125,
      "learning_rate": 0.0034316353887399466,
      "loss": 1.4042,
      "step": 4900
    },
    {
      "epoch": 0.6702412868632708,
      "grad_norm": 1612.093017578125,
      "learning_rate": 0.0032975871313672923,
      "loss": 1.4,
      "step": 5000
    },
    {
      "epoch": 0.6836461126005362,
      "grad_norm": 3070.74609375,
      "learning_rate": 0.003163538873994638,
      "loss": 1.4078,
      "step": 5100
    },
    {
      "epoch": 0.6970509383378016,
      "grad_norm": 1099.935302734375,
      "learning_rate": 0.0030294906166219836,
      "loss": 1.407,
      "step": 5200
    },
    {
      "epoch": 0.710455764075067,
      "grad_norm": 3631.2216796875,
      "learning_rate": 0.00289544235924933,
      "loss": 1.4003,
      "step": 5300
    },
    {
      "epoch": 0.7238605898123325,
      "grad_norm": 9309.4326171875,
      "learning_rate": 0.002761394101876676,
      "loss": 1.4108,
      "step": 5400
    },
    {
      "epoch": 0.7372654155495979,
      "grad_norm": 3675.920654296875,
      "learning_rate": 0.0026273458445040215,
      "loss": 1.4034,
      "step": 5500
    },
    {
      "epoch": 0.7506702412868632,
      "grad_norm": 4350.01025390625,
      "learning_rate": 0.002493297587131367,
      "loss": 1.3989,
      "step": 5600
    },
    {
      "epoch": 0.7640750670241286,
      "grad_norm": 4826.0576171875,
      "learning_rate": 0.0023592493297587133,
      "loss": 1.4012,
      "step": 5700
    },
    {
      "epoch": 0.7774798927613941,
      "grad_norm": 7727.0888671875,
      "learning_rate": 0.002225201072386059,
      "loss": 1.3957,
      "step": 5800
    },
    {
      "epoch": 0.7908847184986595,
      "grad_norm": 1305.029296875,
      "learning_rate": 0.002091152815013405,
      "loss": 1.4073,
      "step": 5900
    },
    {
      "epoch": 0.8042895442359249,
      "grad_norm": 1859.386962890625,
      "learning_rate": 0.0019571045576407507,
      "loss": 1.4073,
      "step": 6000
    },
    {
      "epoch": 0.8176943699731903,
      "grad_norm": 1972.5743408203125,
      "learning_rate": 0.0018230563002680966,
      "loss": 1.3996,
      "step": 6100
    },
    {
      "epoch": 0.8310991957104558,
      "grad_norm": 2141.7587890625,
      "learning_rate": 0.0016890080428954425,
      "loss": 1.4106,
      "step": 6200
    },
    {
      "epoch": 0.8445040214477212,
      "grad_norm": 4030.7998046875,
      "learning_rate": 0.0015549597855227882,
      "loss": 1.4028,
      "step": 6300
    },
    {
      "epoch": 0.8579088471849866,
      "grad_norm": 2230.2294921875,
      "learning_rate": 0.0014209115281501343,
      "loss": 1.4022,
      "step": 6400
    },
    {
      "epoch": 0.871313672922252,
      "grad_norm": 5506.3935546875,
      "learning_rate": 0.00128686327077748,
      "loss": 1.4052,
      "step": 6500
    },
    {
      "epoch": 0.8847184986595175,
      "grad_norm": 4707.74462890625,
      "learning_rate": 0.0011528150134048256,
      "loss": 1.3961,
      "step": 6600
    },
    {
      "epoch": 0.8981233243967829,
      "grad_norm": 5372.24072265625,
      "learning_rate": 0.0010187667560321715,
      "loss": 1.4075,
      "step": 6700
    },
    {
      "epoch": 0.9115281501340483,
      "grad_norm": 2253.900146484375,
      "learning_rate": 0.0008847184986595175,
      "loss": 1.4076,
      "step": 6800
    },
    {
      "epoch": 0.9249329758713136,
      "grad_norm": 1837.6939697265625,
      "learning_rate": 0.0007506702412868633,
      "loss": 1.3962,
      "step": 6900
    },
    {
      "epoch": 0.938337801608579,
      "grad_norm": 510.4534912109375,
      "learning_rate": 0.0006166219839142091,
      "loss": 1.3971,
      "step": 7000
    },
    {
      "epoch": 0.9517426273458445,
      "grad_norm": 40911.41796875,
      "learning_rate": 0.0004825737265415549,
      "loss": 1.3941,
      "step": 7100
    },
    {
      "epoch": 0.9651474530831099,
      "grad_norm": 3324.248046875,
      "learning_rate": 0.00034852546916890086,
      "loss": 1.3926,
      "step": 7200
    },
    {
      "epoch": 0.9785522788203753,
      "grad_norm": 1801.048095703125,
      "learning_rate": 0.00021447721179624666,
      "loss": 1.3928,
      "step": 7300
    },
    {
      "epoch": 0.9919571045576407,
      "grad_norm": 1019.1266479492188,
      "learning_rate": 8.04289544235925e-05,
      "loss": 1.3934,
      "step": 7400
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.2515625,
      "eval_loss": 1.3838558197021484,
      "eval_runtime": 2.6163,
      "eval_samples_per_second": 244.621,
      "eval_steps_per_second": 3.822,
      "step": 7460
    }
  ],
  "logging_steps": 100,
  "max_steps": 7460,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.0332814450087424e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
